{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c95b5ca-e660-4c89-91f0-6f393e325f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile('C:/Users/91971/Desktop/Python Notebooks/NLP Project/archive.zip') # Add path to zip file \n",
    "df = pd.read_csv(zf.open('kaggle_RC_2019-05.csv')) # Add name of .csv file tp read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3397f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91971\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary models and libraries for the Pre Processing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer() # For lemmatization of text \n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"\\w{2,}\") # For tokenization, matches two or more word characters consecutively\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English() # For providing information about the English words in text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adb85200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['body']) # Drop NA valued 'body' rows\n",
    "\n",
    "# Convert all strings in 'body' to lowercase and replace all the characters matching the following regex with no space\n",
    "df['body'] = df['body'].str.lower() \n",
    "df['body'] = df['body'].str.replace(r\"\"\"([@&]\\w+.*?[:;]?\"|â€œ.*?|â€.*?|[()\\[\\]{}*+\\|.,;:<>!#$%^_=~-])\"\"\", '')\n",
    "\n",
    "# Drop columns of no use\n",
    "df = df.drop('controversiality',axis=1)\n",
    "df = df.drop('score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1fd1ce32-3ab8-48e9-beb7-abe31f5cf447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all rows of the dataset to filter the 'body' column\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    # Tokenize the 'body' \n",
    "    tokenize_body = tokenizer.tokenize(df['body'].iloc[i])\n",
    "\n",
    "    # Remove stop words and punctuations(if any are left after tokenization) from 'body'\n",
    "    new_body = []\n",
    "    final_body = []\n",
    "    for word in tokenize_body:\n",
    "        word = lemma.lemmatize(word)\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if (lexeme.is_stop == False) and (lexeme.is_punct == False):\n",
    "            new_body.append(word)\n",
    "    for word in new_body:\n",
    "        word = lemma.lemmatize(word)\n",
    "        final_body.append(word)\n",
    "\n",
    "    # Join all filtered tokens of 'body' into a new string\n",
    "    df.at[i,'body'] = ' '.join(final_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d214220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the filtered text based on 'subreddit' column by joining all text beloning to same entry\n",
    "df['body'] = df.groupby('subreddit')['body'].transform(lambda x: ' '.join(x))\n",
    "\n",
    "# Drop the redundant rows \n",
    "df.drop_duplicates(subset=['subreddit'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Pre Processed data to a .csv file \n",
    "df.to_csv('pre-processed-data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
