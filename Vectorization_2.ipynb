{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "onj = SentimentIntensityAnalyzer()\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import chardet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = [\n",
    "    \"gameofthrones\",\n",
    "    \"aww\",\n",
    "    \"gaming\",\n",
    "    \"news\",\n",
    "    \"politics\",\n",
    "    \"dankmemes\",\n",
    "    \"relationship_advice\",\n",
    "    \"nba\",\n",
    "    \"worldnews\",\n",
    "    \"AskReddit\",\n",
    "    \"AmItheAsshole\",\n",
    "    \"SquaredCircle\",\n",
    "    \"The_Donald\",\n",
    "    \"leagueoflegends\",\n",
    "    \"hockey\",\n",
    "    \"videos\",\n",
    "    \"teenagers\",\n",
    "    \"gonewild\",\n",
    "    \"movies\",\n",
    "    \"funny\",\n",
    "    \"pics\",\n",
    "    \"marvelstudios\",\n",
    "    \"memes\",\n",
    "    \"soccer\",\n",
    "    \"freefolk\",\n",
    "    \"MortalKombat\",\n",
    "    \"todayilearned\",\n",
    "    \"apexlegends\",\n",
    "    \"asoiaf\",\n",
    "    \"Market76\",\n",
    "    \"Animemes\",\n",
    "    \"FortNiteBR\",\n",
    "    \"nfl\",\n",
    "    \"trashy\",\n",
    "    \"unpopularopinion\",\n",
    "    \"ChapoTrapHouse\",\n",
    "    \"RoastMe\",\n",
    "    \"Showerthoughts\",\n",
    "    \"wallstreetbets\",\n",
    "    \"Pikab\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_dict = {subreddit[i]:i for i in range(len(subreddit))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = chardet.detect(file.read())\n",
    "    return result['encoding']\n",
    "\n",
    "file_path = 'pre-processed-data.csv'\n",
    "detected_encoding = detect_encoding(file_path)\n",
    "\n",
    "df = pd.read_csv(file_path, encoding=detected_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.drop(columns=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = {'Subjectivity':list(),'Polarity':[],'Neg':[],'Pos':[],'Compound':[],'Complexity':[],'Class':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    word = word.lower().strip()\n",
    "    vowel_sounds = re.findall(r'[aeiouy]+', word)\n",
    "    syllables = len(vowel_sounds)\n",
    "    if word.endswith('e'):\n",
    "        syllables -= 1\n",
    "    if word.endswith('y') and not re.match(r'[aeiouy]+y$', word):\n",
    "        syllables += 1\n",
    "    return syllables\n",
    "\n",
    "def flesch_kincaid_grade_level(text):\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'[.?!]+', text.strip())\n",
    "\n",
    "    avg_syllables = sum(count_syllables(word) for word in words) / len(words)\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "    fkgl = 0.39 * avg_words_per_sentence + 11.8 * avg_syllables - 15.59\n",
    "    return round(fkgl, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(df.shape[0]):\n",
    "    text = str(df.loc[i]['body'])\n",
    "    blob = TextBlob(text)\n",
    "    df_data['Subjectivity'].append(blob.subjectivity)\n",
    "    df_data['Polarity'].append(blob.sentiment.polarity)\n",
    "    polarity_scores = onj.polarity_scores(text)\n",
    "    df_data['Neg'].append(polarity_scores['neg'])\n",
    "    df_data['Pos'].append(polarity_scores['pos'])\n",
    "    df_data['Compound'].append(polarity_scores['compound'])\n",
    "    fkgl = flesch_kincaid_grade_level(text)\n",
    "    df_data['Complexity'].append(fkgl)\n",
    "    df_data['Class'].append(df.loc[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ = pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_.to_csv('Mid_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Mid_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.iloc[:,:6]\n",
    "labels = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_medians = features.groupby(labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_params(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity_scores = onj.polarity_scores(text)\n",
    "    fkgl = flesch_kincaid_grade_level(text)\n",
    "    return [blob.subjectivity,blob.sentiment.polarity,polarity_scores['neg'],polarity_scores['pos'],polarity_scores['compound'],fkgl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(data_point1, data_point2):\n",
    "    dot_product = np.dot(data_point1, data_point2)\n",
    "    norm1 = np.linalg.norm(data_point1)\n",
    "    norm2 = np.linalg.norm(data_point2)\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = str(input(\"Enter the post whose subreddit you want to find: \"))\n",
    "test_data_point = return_params(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = {}\n",
    "for i in range(class_medians.shape[0]):\n",
    "    similarity[i] = cosine_similarity(test_data_point,list(class_medians.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_elements = sorted(similarity.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ishaan's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict={}\n",
    "\n",
    "with open(\"pre-processed-data_`.csv\", \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "\n",
    "    k=f.readline()\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        k=f.readline()\n",
    "        \n",
    "        if len(k)==0:\n",
    "            break\n",
    "\n",
    "        _,x,k=k.split(\",\",2)\n",
    "\n",
    "        k=re.sub('[\\d|\\_]', '', k)\n",
    "\n",
    "        token_dict[x]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "dense=tfs.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names, index= list(token_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_lists = {i:subreddit[i] for i in range(len(subreddit))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsubs =[subreddit_lists[top3_elements[i][0]] for i in range(len(top3_elements))]\n",
    "\n",
    "impwords={}\n",
    "\n",
    "for  subreddit in inputsubs:\n",
    "    \n",
    "    s = pd.Series(df.loc[subreddit])\n",
    "    impwords[subreddit]=s[s > 0.0001].sort_values(ascending=False)[:100].keys().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matching_words(words, text):\n",
    "  count = 0\n",
    "  for word in text.lower().split():\n",
    "    if word in words:\n",
    "      count += 1\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_prob_sub = 0\n",
    "count = 0\n",
    "for i in impwords:\n",
    "    if count<count_matching_words(impwords[i],test_text):\n",
    "        count = count_matching_words(impwords[i],test_text)\n",
    "        most_prob_sub = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teenagers'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(most_prob_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You dont need to add the next part, it is for further extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "\n",
    "def analyze_text(text):\n",
    "    \"\"\"\n",
    "    Analyzes a text and returns a dictionary containing values for 6 dimensions:\n",
    "        - joy\n",
    "        - anger\n",
    "        - complexity (average number of syllables per word)\n",
    "    \"\"\"\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Calculate emotion scores (range: 0-1)\n",
    "    joy = blob.sentiment.polarity\n",
    "    anger = blob.sentiment.subjectivity\n",
    "\n",
    "    # Calculate word complexity (average syllables per word)\n",
    "    syllables = sum(count_syllables(word) for word in text.split())\n",
    "    word_count = len(text.split())\n",
    "    complexity = syllables / word_count if word_count else 0\n",
    "\n",
    "\n",
    "    # Return dictionary with analysis results\n",
    "    return {\n",
    "        \"joy\": joy,\n",
    "        \"anger\": anger,\n",
    "        \"complexity\": complexity,\n",
    "    }\n",
    "\n",
    "\n",
    "def count_syllables(word):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    word = word.lower().strip()\n",
    "\n",
    "    # Count vowel sounds\n",
    "    vowel_sounds = re.findall(r\"[aeiouy]+\", word)\n",
    "\n",
    "    # Count syllables (assume consonant sounds between vowel sounds)\n",
    "    syllables = len(vowel_sounds)\n",
    "\n",
    "    # Special cases for silent \"e\" and \"y\"\n",
    "    if word.endswith(\"e\"):\n",
    "        syllables -= 1\n",
    "    if word.endswith(\"y\") and not re.match(r\"[aeiouy]+y$\", word):\n",
    "        syllables += 1\n",
    "\n",
    "    return syllables\n",
    "\n",
    "\n",
    "def flesch_kincaid_grade_level(text):\n",
    "\n",
    "    words = text.split()\n",
    "    sentences = re.split(r\"[.?!]+\", text.strip())\n",
    "\n",
    "    avg_syllables = sum(count_syllables(word) for word in words) / len(words)\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "    fkgl = 0.39 * avg_words_per_sentence + 11.8 * avg_syllables - 15.59\n",
    "\n",
    "    return round(fkgl, 2)\n",
    "\n",
    "text = \"Supercalifragilisticexpialidocious\"\n",
    "analysis_data = analyze_text(text)\n",
    "\n",
    "print(analysis_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
